{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from A3.py file found on the Computer Vision canvas files page, used to crop images following registraion\n",
    "\n",
    "def minbbox(element):\n",
    "    \"\"\"\n",
    "    Takes in a binary mask and returns a minimal bounding box around\n",
    "    the non-zero elements.\n",
    "    Input:\n",
    "    - mask: a mask image (should contain only 0 and non-zero elements)\n",
    "    Returns:\n",
    "    - bbox: a list of four points in the form: [min_x, min_y, max_x, max_y]\n",
    "    \"\"\"\n",
    "\n",
    "    vals = np.nonzero(element)\n",
    "    min_h = vals[0].min()\n",
    "    max_h = vals[0].max()\n",
    "    min_w = vals[1].min()\n",
    "    max_w = vals[1].max()\n",
    "\n",
    "    return [min_w, min_h, max_w, max_h]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prep images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_fn is a list of file paths corresponding to a scene captured at varying exposure levels\n",
    "# exposure levels corresponding to the current set of images can be found in Exposureinfo.txt\n",
    "img_fn = [\"data/8/img-1.tif\", \"data/8/img-2.tif\", \"data/8/img-3.tif\", \"data/8/img-4.tif\", \"data/8/img-5.tif\", \"data/8/img-6.tif\", \"data/8/img-7.tif\", \"data/8/img-8.tif\", \"data/8/img-9.tif\"]\n",
    "exposure_times = np.array([(1/8000), (1/4000), (1/2000), (1/1000), (1/400), (1/200), (1/250),(1/500), (1/125), (1/60), (1/30), (1/15), (1/6)], dtype=np.float32)\n",
    "\n",
    "img_list = [cv.imread(fn) for fn in img_fn]\n",
    "log_exposure = [np.log(x) for x in exposure_times]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all images in img_list from RGB to grayscale\n",
    "gray = [cv.cvtColor(img, cv.COLOR_RGB2GRAY) for img in img_list]\n",
    "\n",
    "# Normalize all gray scale images using histogram equalization\n",
    "norm = [cv.equalizeHist(test) for test in gray]\n",
    "\n",
    "# Use the first normalized image as the reference image for alignment\n",
    "img1 = norm[0]\n",
    "\n",
    "# Initalize the output list with the first image (unaltered)\n",
    "output = [img_list[0]]\n",
    "\n",
    "# Loop through the rest of the normalized images to align them to the reference image\n",
    "for i in range(1, len(norm)):\n",
    "    print(i) # Printing the current image index\n",
    "    img2 = norm[i] # Storing the current normalized image in img2\n",
    "\n",
    "    # Create an instance of the AKAZE feature detector\n",
    "    detector = cv.AKAZE_create()\n",
    "\n",
    "    # Detecting and computing keypoints for the reference and current images\n",
    "    kp1, des1 = detector.detectAndCompute(img1, None) # Reference image key points\n",
    "    kp2, des2 = detector.detectAndCompute(img2, None) # Current image key points\n",
    "\n",
    "    # Brute force matching the detected keypoints between the reference and current image key points\n",
    "    bf = cv.BFMatcher(cv.NORM_HAMMING, crossCheck = True)\n",
    "    matches = bf.match(des1, des2)\n",
    "\n",
    "    # Storing the matched key points (sorted based on distance)\n",
    "    matches = sorted(matches, key = lambda x: x.distance)\n",
    "\n",
    "    # Keeping the top 75% of matches or at least 10 matches\n",
    "    num_good_matches = int(len(matches) * 0.75)\n",
    "    matches = matches[:max(num_good_matches, 10)]\n",
    "\n",
    "    # Getting the coordinates corresponding to the matches from both images\n",
    "    pts1 = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 2)\n",
    "    pts2 = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 2)\n",
    "\n",
    "    # Compute the transofrmation matrix\n",
    "    transformation_matrix, inliers = cv.findHomography(\n",
    "                pts2, pts1, method=cv.RANSAC, ransacReprojThreshold=3.0, confidence=0.99\n",
    "            )\n",
    "\n",
    "    # Warping the current image to align with the reference image via the transformation matrix\n",
    "    aligned_img = cv.warpPerspective(\n",
    "                img_list[i], \n",
    "                transformation_matrix, \n",
    "                (img1.shape[1], img1.shape[0]),\n",
    "                flags=cv.INTER_LINEAR\n",
    "            )\n",
    "    # Add the aligned image to output\n",
    "    output += [aligned_img]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop registered images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set crop offset\n",
    "offset = 300\n",
    "\n",
    "masks = []\n",
    "\n",
    "# Loop through each of the aligned images\n",
    "for img in output:\n",
    "    minw, minh, maxw, maxh = minbbox(img) # Get the minimum bounding box for each aligned image\n",
    "    h, w, _ = img.shape\n",
    "    mask = np.zeros((h,w), dtype='uint8')\n",
    "\n",
    "    # Fill the mask based on the dimensions of the minimum bounding box\n",
    "    mask[minh:maxh, minw:maxw] = np.ones(((maxh-minh), (maxw-minw)), dtype='uint8')\n",
    "    masks += [mask]\n",
    "\n",
    "# The first mask is the reference since it acted as the reference image during the alignment process\n",
    "joined = masks[0]\n",
    "for mask in masks[1:]:\n",
    "    # Updating the composite mask with each iteration using a element wise logical and\n",
    "    joined = np.logical_and(joined, mask)\n",
    "minw, minh, maxw, maxh = minbbox(joined)\n",
    "\n",
    "# Shrinking the image with the hyperparameter \"offset\" to remove any residuals\n",
    "minw = minw + int(offset / 2)\n",
    "maxw = maxw - int(offset / 2)\n",
    "minh = minh + int(offset / 2)\n",
    "maxh = maxh - int(offset / 2)\n",
    "\n",
    "cropped = []\n",
    "\n",
    "# Cropping and saving each of the aligned images\n",
    "for el in output:\n",
    "    cropped += [el[minh:maxh, minw:maxw]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debevec HDR compositing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and prep samples for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100\n",
    "\n",
    "samples = [(random.randint(0, cropped[0].shape[0]), random.randint(0, cropped[0].shape[1])) for _ in range(num_samples)]\n",
    "\n",
    "# Splitting the three channels into seperate color channels\n",
    "red = [img[:,:,0] for img in cropped]\n",
    "green = [img[:,:,1] for img in cropped]\n",
    "blue = [img[:,:,2] for img in cropped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing arrays to hold the sampled red channel values\n",
    "redsample = np.zeros((num_samples, len(cropped)))\n",
    "for j in range(len(cropped)):\n",
    "    for i in range(len(samples)):\n",
    "        redsample[i,j] = red[j][samples[i][0], samples[i][1]]\n",
    "\n",
    "# Initializing arrays to hold the sampled green channel values\n",
    "greensample = np.zeros((num_samples, len(cropped)))\n",
    "for j in range(len(cropped)):\n",
    "    for i in range(len(samples)):\n",
    "        greensample[i,j] = green[j][samples[i][0], samples[i][1]]\n",
    "\n",
    "# Initializing arrays to hold the sampled blue channel values\n",
    "bluesample = np.zeros((num_samples, len(cropped)))\n",
    "for j in range(len(cropped)):\n",
    "    for i in range(len(samples)):\n",
    "        bluesample[i,j] = blue[j][samples[i][0], samples[i][1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve for the response function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm mased on equation 3 of Debevec SIGGRAPH '97, translated from appendix code\n",
    "\n",
    "def gsolve(Z, B, l, w, midval):\n",
    "    \"\"\"\n",
    "    - Z: array of pixel values, Z[i,j] is the pixel value at location i in image j\n",
    "    - B: array of log delta t (shutter speeds), B[j] for image j\n",
    "    - l: lambda, the constant that determines the amount of smoothness\n",
    "    - w: weighting function or array, w[z] gives the weight for pixel value z\n",
    "    \"\"\"\n",
    "    n = 256\n",
    "\n",
    "    A = np.zeros((Z.shape[0] * Z.shape[1] + n + 1, n + Z.shape[0]))\n",
    "    b = np.zeros((A.shape[0], 1))\n",
    "    \n",
    "\n",
    "    k = 0 \n",
    "    for i in range(Z.shape[0]):\n",
    "        for j in range(Z.shape[1]):\n",
    "            z_val = Z[i, j]\n",
    "            wij = w(z_val)\n",
    "            A[k, int(z_val)] = wij\n",
    "            A[k, n + i] = -wij\n",
    "            b[k, 0] = wij * B[j]  \n",
    "            k += 1\n",
    "    \n",
    "\n",
    "    A[k, midval] = 1\n",
    "    k += 1\n",
    "    \n",
    "    for i in range(n - 2):\n",
    "\n",
    "        wval = w(i + 1)\n",
    "\n",
    "        \n",
    "        A[k, i] = l * wval\n",
    "        A[k, i + 1] = -2 * l * wval\n",
    "        A[k, i + 2] = l * wval\n",
    "        k += 1\n",
    "    \n",
    "    x = np.linalg.lstsq(A, b, rcond=None)[0]\n",
    "\n",
    "    g = x[:n].flatten()\n",
    "    lE = x[n:].flatten()\n",
    "    \n",
    "    return g, lE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center-priority weighting function, taken from equation 4 of Debevec and Malik\n",
    "\n",
    "def weight_function(z, z_min=0, z_max=255):\n",
    "    \"\"\"\n",
    "    Implements the triangular weighting function as described in the equation:\n",
    "    \n",
    "    w(z) = {\n",
    "        z - z_min       for z <= (z_min + z_max)/2\n",
    "        z_max - z       for z > (z_min + z_max)/2\n",
    "    }\n",
    "\n",
    "    \"\"\"\n",
    "    mid_point = 0.5 * (z_min + z_max)\n",
    "    \n",
    "    if np.isscalar(z):\n",
    "        if z <= mid_point:\n",
    "            return z - z_min\n",
    "        else:\n",
    "            return z_max - z\n",
    "    else:\n",
    "        z = np.asarray(z)\n",
    "        weights = np.zeros_like(z, dtype=float)\n",
    "        \n",
    "        mask_lower = z <= mid_point\n",
    "        weights[mask_lower] = z[mask_lower] - z_min\n",
    "        \n",
    "        mask_upper = z > mid_point\n",
    "        weights[mask_upper] = z_max - z[mask_upper]\n",
    "        \n",
    "        return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a radiance map, applying equation 6 from Debevec and Malik to each pixel of the scene\n",
    "\n",
    "def create_radiance_map(images, g, exposure_times, weights):\n",
    "    radiance_map = np.zeros(images[0].shape)\n",
    "\n",
    "    # Iterate through pixels\n",
    "    for i in range(radiance_map.shape[0]):\n",
    "        if i % 100 == 0:\n",
    "            print(str(i) + \" of \" + str(radiance_map.shape[0]))\n",
    "        for j in range(radiance_map.shape[1]):\n",
    "            pixel_values = [images[k][i,j] for k in range(len(images))]\n",
    "            \n",
    "            # Equation 6\n",
    "            numerator = 0\n",
    "            denominator = 0\n",
    "            for k in range(len(images)):\n",
    "                w = weights(pixel_values[k])\n",
    "                numerator += w * (g[pixel_values[k]] - exposure_times[k])\n",
    "                denominator += w\n",
    "            \n",
    "            # Compute log radiance\n",
    "            if denominator > 0:\n",
    "                radiance_map[i,j] = numerator / denominator\n",
    "            else:\n",
    "                radiance_map[i,j] = g[pixel_values[0]] - exposure_times[0]\n",
    "    \n",
    "    # Convert from log space to linear space\n",
    "    radiance_map = np.exp(radiance_map)\n",
    "    return radiance_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate response functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rg, _) = gsolve(redsample, log_exposure,40, weight_function, 160)\n",
    "(gg, _) = gsolve(greensample, log_exposure,40, weight_function, 160)\n",
    "(bg, _) = gsolve(bluesample, log_exposure,40, weight_function, 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the camera response function\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(rg, range(256), color = \"red\")\n",
    "plt.plot(gg, range(256), color = \"green\")\n",
    "plt.plot(bg, range(256), color = \"blue\")\n",
    "plt.ylabel('Pixel Value')\n",
    "plt.xlabel('Log Exposure')\n",
    "plt.title('Camera Response Function')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate radiance maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = create_radiance_map(red, rg, log_exposure, weight_function)\n",
    "gr = create_radiance_map(green, gg, log_exposure, weight_function)\n",
    "br = create_radiance_map(blue, bg, log_exposure, weight_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tonemap and final processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reinhard tonemapping function adapted from https://github.com/deepankarc/hdr-imaging/tree/master\n",
    "\n",
    "def reinhard_tonemap(radiance_map, gamma=1/2.2, alpha=0.25):\n",
    "    # tone mapping parameters\n",
    "    E_map = np.zeros_like(radiance_map)\n",
    "\n",
    "    # normalize radiance_map map for each channel\n",
    "    for ch in range(3):\n",
    "        map_channel = radiance_map[:,:,ch]\n",
    "        E_map[:,:,ch] = (map_channel - np.min(map_channel)) / (np.max(map_channel) - np.min(map_channel))\n",
    "\n",
    "    # gamma correction\n",
    "    E_map = E_map**gamma\n",
    "\n",
    "    # convert to grayscale and apply Reinhart Tone Mapping\n",
    "    L = cv.cvtColor(E_map.astype('float32'), cv.COLOR_RGB2GRAY)\n",
    "    L_avg = np.exp(np.mean(np.log(L))) # average normalized grayscale radiance (Equation 1)\n",
    "    T = alpha / L_avg * L # Equation 2\n",
    "    L_tone = T * (1 + (T / (T.max())**2)) / (1 + T) # Equation 4\n",
    "    M = L_tone / L # Ratio between the tone mapping and original normalized map\n",
    "\n",
    "    # scale each channel\n",
    "    tonemapped_img = np.empty_like(E_map)\n",
    "    for ch in range(3):\n",
    "        tonemapped_img[:,:,ch] = E_map[:,:,ch] * M\n",
    "\n",
    "    tonemapped_img = np.clip(tonemapped_img, 0.0, 1.0)\n",
    "    return (tonemapped_img * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine separate radiance maps to a single multi-channel image and tonemap\n",
    "\n",
    "r_test = np.zeros((rr.shape[0], rr.shape[1], 3))\n",
    "r_test[:,:,0] = rr\n",
    "r_test[:,:,1] = gr\n",
    "r_test[:,:,2] = br\n",
    "\n",
    "reinhard_out = reinhard_tonemap(r_test, 1/1.4, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply bloom to image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image\n",
    "img = reinhard_out\n",
    "gray = cv.cvtColor(img, cv.COLOR_RGB2GRAY)\n",
    "\n",
    "white_threshold=0.7\n",
    "scale_factor=0.34\n",
    "blur_strength=1001\n",
    "\n",
    "# Normalize the image to range [0, 1]\n",
    "normalized_image = img / 255.0\n",
    "gray = gray / 255.0\n",
    "# Define a threshold for \"white\" (close to 1 for all RGB channels)\n",
    "# white_mask = np.any(normalized_image > white_threshold, axis=-1)\n",
    "white_mask = gray > white_threshold\n",
    "\n",
    "# Scale up the \"white\" regions\n",
    "# bloom_mask = normalized_image * white_mask[..., np.newaxis] * scale_factor\n",
    "bloom_mask =  white_mask * scale_factor\n",
    "\n",
    "# Apply Gaussian blur to simulate glow\n",
    "bloom_mask = cv.GaussianBlur(bloom_mask, (blur_strength, blur_strength), 0)\n",
    "\n",
    "# Dilating the bloom mask after the Gaussian blur\n",
    "kern = cv.getStructuringElement(cv.MORPH_ELLIPSE, (5,5))\n",
    "bloom_mask = cv.dilate(bloom_mask, kern, iterations=2)\n",
    "\n",
    "cv.imwrite(\"bloom_mask.jpg\", (bloom_mask * 255).astype(np.uint8))\n",
    "\n",
    "\n",
    "# Merge the bloom effect with the original image\n",
    "result = np.clip(normalized_image + bloom_mask[...,np.newaxis], 0, 1)\n",
    "\n",
    "# Convert back to 8-bit image\n",
    "result = (result * 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "cv.imwrite(\"gray_rein_bloomed_image.jpg\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# White balance image using simple gray-world algorithm\n",
    "# Adapted from https://medium.com/@ricodelrosario/snap-up-your-images-with-python-a-tutorial-on-white-balancing-histogram-manipulation-and-fourier-3e67df45b47\n",
    "\n",
    "cv.imwrite(\"balance.jpg\",((result * (result.mean() / result.mean(axis=(0, 1))))\n",
    "            .clip(0, 255).astype(int)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
